{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import argparse\n",
    "import hashlib\n",
    "import os\n",
    "%matplotlib inline\n",
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset and encoding the string attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('./class-Neo4j.csv')\n",
    "print(df.shape)\n",
    "X= df.iloc[:,0:97]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "#CSV FIlE CONVERSION\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_raw = df[['Hash','LongName']]\n",
    "X = pd.DataFrame()\n",
    "for col in X_raw:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X_raw[col].astype(str))\n",
    "\n",
    "new_X=df.iloc[:,2:97]\n",
    "data_new = pd.concat([X, new_X], ignore_index=False, sort=False, axis=1)\n",
    "# data_new.fillna(0, inplace=True)\n",
    "\n",
    "data_new = data_new.iloc[:,0:97]\n",
    "\n",
    "for column in data_new.columns:\n",
    "    maxval = data_new[column].max()\n",
    "    minval = data_new[column].min()    \n",
    "    if(maxval == minval):\n",
    "        data_new[column] = (data_new[column] - data_new[column].min()) / (data_new[column].max() - data_new[column].min()+ .000001)        \n",
    "    else:\n",
    "        data_new[column] = (data_new[column] - data_new[column].min()) / (data_new[column].max() - data_new[column].min())\n",
    "\n",
    "data = pd.concat([data_new, y], ignore_index=False, sort=False, axis=1) \n",
    "data['Bugs'] = data['Bugs'].apply(lambda x : 1 if x != 0 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def crossover(parents, offspring_size):\n",
    "    offspring = numpy.empty(offspring_size)\n",
    "    crossover_point = numpy.uint8(offspring_size[1]/2)\n",
    "    for k in range(offspring_size[0]):\n",
    "        parent1_idx = k%parents.shape[0]\n",
    "        parent2_idx = (k+1)%parents.shape[0]\n",
    "        offspring[k, 0:crossover_point] = parents.iloc[parent1_idx, 0:crossover_point]\n",
    "        offspring[k, crossover_point:] = parents.iloc[parent2_idx, crossover_point:]\n",
    "    return offspring\n",
    "\n",
    "def mutation(offspring_crossover, num_mutations=1):\n",
    "    mutations_counter = numpy.uint8(offspring_crossover.shape[1] / num_mutations)\n",
    "    print('\\n')\n",
    "    for idx in range(offspring_crossover.shape[0]):\n",
    "        gene_idx = mutations_counter - 1\n",
    "        for mutation_num in range(num_mutations):\n",
    "            random_value = numpy.random.uniform(-1.0, 1.0, 1)\n",
    "            offspring_crossover[idx, gene_idx] = offspring_crossover[idx, gene_idx] + random_value\n",
    "            gene_idx = gene_idx + mutations_counter\n",
    "    return offspring_crossover\n",
    "\n",
    "\n",
    "def select_mating_pool(population, highest_point, num_parents):\n",
    "    parents = numpy.empty((num_parents, new_population.shape[1]))\n",
    "    for parent_num in range(num_parents):\n",
    "\n",
    "        max_fitness_idx = numpy.where(fitness == numpy.max(fitness))\n",
    "        max_fitness_idx = max_fitness_idx[0][0]\n",
    "        parents[parent_num, :] = new_population.iloc[max_fitness_idx, :]\n",
    "        fitness[max_fitness_idx] = -99999999999\n",
    "\n",
    "    print('parents: ')\n",
    "    print(parents)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK STARTS\n",
    "#Inputs\n",
    "num_weights = 97\n",
    "population = data.sample(frac=.1) \n",
    "# print(population)\n",
    "member_in_pop = population.shape[0]\n",
    "print(member_in_pop)\n",
    "pop_size = (member_in_pop, num_weights)\n",
    "\n",
    "num_parents_mating = 30\n",
    "\n",
    "\n",
    "new_population = population.iloc[:,0:97]\n",
    "num_generations = 100\n",
    "#BLOCK ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# BLOCK STARTS\n",
    "#This function is for calculating distance\n",
    "def distance(p0, p1):\n",
    "    \"\"\" Calculate the distance between two hydrant \"\"\"\n",
    "    dist = math.sqrt((p0[0] - p1[0])**2 + (p0[1] - p1[1])**2)\n",
    "    return dist\n",
    "# BLOCK ENDS\n",
    "\n",
    "\n",
    "\n",
    "pop_multi = numpy.random.uniform(low=0, high=1.0, size=pop_size)\n",
    "dataframe_new= DataFrame(pop_multi)\n",
    "# print(pop_multi)\n",
    "\n",
    "def cal_pop_fitness(dataframe_new,population):\n",
    "    pop_multi = pd.DataFrame(dataframe_new).to_numpy()\n",
    "    print('\\n')\n",
    "    fitness = []\n",
    "    highest_point = []\n",
    "    high_fit = 0\n",
    "    weight = []\n",
    "    temp_population = DataFrame()\n",
    "    j=0\n",
    "    parent = numpy.ones((1, num_weights))\n",
    "    temp_fit = []\n",
    "    index = []\n",
    "    data_new = []\n",
    "    tsnelist = []\n",
    "    tsne_df = []\n",
    "    res = zip()\n",
    "    \n",
    "    for i in range(member_in_pop):\n",
    "        j=j+1\n",
    "#         print(i)\n",
    "        #pop\n",
    "        x= population.iloc[:,0:97].multiply(pop_multi[i], axis=1)\n",
    "        y=population.iloc[:,-1]\n",
    "        data = pd.concat([x, y], ignore_index=False, sort=False, axis=1)\n",
    "        data_new.append(data)\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(n_components=2, random_state=0, perplexity=7.0)\n",
    "        tsne_obj= tsne.fit_transform(data)\n",
    "        tsne_df1 = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                            'Y':tsne_obj[:,1],\n",
    "                           'bug':population.iloc[:,-1]})\n",
    "        tsnelist.append(tsne_df1)\n",
    "        \n",
    "        X=tsne_df1.drop('bug', axis=1)\n",
    "        y=tsne_df1['bug']\n",
    "\n",
    "        forest = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-5))\n",
    "        cv_results = cross_val_score(forest,X,y,cv=10)\n",
    "        forest.fit(X, y)\n",
    "\n",
    "        prediction = cross_val_predict(forest,X,y,cv=10)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y, prediction)  \n",
    "        precision = precision_score(y, prediction)\n",
    "        recall = recall_score(y, prediction, average='binary')\n",
    " \n",
    "        temp_fit.append(recall)\n",
    "        index.append(i)\n",
    "\n",
    "    res = zip(temp_fit,index)\n",
    "    result = list(res)\n",
    "    \n",
    "    if(high_fit<max(temp_fit)):\n",
    "        high_fit = max(temp_fit)\n",
    "        fitness=temp_fit\n",
    "        \n",
    "    for i in range(member_in_pop):     \n",
    "        if(high_fit == result[i][0]):\n",
    "            j = result[i][1]\n",
    "            temp_population = data_new[j]\n",
    "            tsne_df = tsnelist[j]\n",
    "            weight = pop_multi[j]\n",
    "    \n",
    "    \n",
    "    ind = np.argpartition(temp_fit, -num_parents_mating)[-num_parents_mating:]\n",
    "    \n",
    "#     print('ind: ', ind)\n",
    "    for i in ind:\n",
    "        j = result[i][1]\n",
    "        parent = np.vstack([parent, pop_multi[j]])\n",
    "    \n",
    "     \n",
    "    parents = pd.DataFrame(parent)\n",
    "    \n",
    "    \n",
    "    return fitness, parents, tsne_df,weight, temp_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "from pandas import DataFrame\n",
    "\n",
    "# print(new_population)\n",
    "\n",
    "best_outputs = []\n",
    "weight_list =[]\n",
    "tsne_df = []\n",
    "temp_pop = []\n",
    "fit = []\n",
    "# fig, ax = plt.subplots( num_generations,1,figsize=(40,50))\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    print(\"Generation : \", generation)\n",
    "    \n",
    "    fitness, parent, tsne,weight, temp_population= cal_pop_fitness(dataframe_new,population)\n",
    "    highest_point = parent\n",
    "    weight_list.append(weight)\n",
    "    tsne_df.append(tsne)\n",
    "    temp_pop.append(temp_population)\n",
    "    fit.append(fitness)\n",
    "\n",
    "    offspring_crossover = crossover(highest_point,\n",
    "                                       offspring_size=(abs(pop_size[0]-highest_point.shape[0]), num_weights))\n",
    "\n",
    "    offspring_mutation = mutation(offspring_crossover, num_mutations=2)\n",
    "\n",
    "   \n",
    "    dataframe_new.iloc[0:highest_point.shape[0], :] = highest_point.values\n",
    "    dataframe_new.iloc[highest_point.shape[0]:, :] = offspring_mutation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Important features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "d = temp_pop[0]\n",
    "print(d)\n",
    "X = d.iloc[:, 0:97]\n",
    "y = d['Bugs']\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest.fit(X,y)\n",
    "\n",
    "feature_imp = pd.Series(forest.feature_importances_,index=data.iloc[:,0:97].columns).sort_values(ascending=False)\n",
    "print(feature_imp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plotting\n",
    "\n",
    "feature = DataFrame(feature_imp)\n",
    "temp_list = []\n",
    "feature_cols = list(feature.iloc[0:30].index)\n",
    "for i in range(num_generations):\n",
    "    temp = temp_pop[i]\n",
    "    bug = temp['Bugs']\n",
    "    \n",
    "    droplist = [i for i in temp.columns if i not in feature_cols]\n",
    "    temp_new = temp.drop(droplist,axis=1)\n",
    "    \n",
    "#     new.dropna(axis=1,inplace=True)\n",
    "#     new = temp_new\n",
    "    for column in temp_new.columns:\n",
    "        temp_new[column] = ((temp_new[column] - temp_new[column].min()) / (temp_new[column].max() - temp_new[column].min()))\n",
    "\n",
    "    temp_new.fillna(0.0, inplace = True)\n",
    "\n",
    "    temp_new=temp_new.applymap(lambda x: x + np.random.rand()/20.0)\n",
    "\n",
    "    new = pd.concat([temp_new, bug],ignore_index=False, sort=False, axis=1) \n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Plot Number = %.i\" % i, fontsize=15)\n",
    "    sns.scatterplot(x=\"X\", y=\"Y\",hue=\"bug\",data=tsne_df[i])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    pd.plotting.parallel_coordinates(\n",
    "        new, 'Bugs', color=('blue', 'red'), alpha=0.5, axvlines=False\n",
    "    )\n",
    "    \n",
    "    plt.xticks(rotation=70)\n",
    "    plt.xlabel(\"Feature Names\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Finding the Feature Values of Each Data Point\")\n",
    "    L=plt.legend()\n",
    "    L.get_texts()[0].set_text('Non-Bug')\n",
    "    L.get_texts()[1].set_text('Bug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "new = temp_pop[28]\n",
    "feature_cols = list(feature.iloc[0:30].index)\n",
    "temp = temp_pop[i]\n",
    "bug = temp['Bugs']\n",
    "    \n",
    "droplist = [i for i in temp.columns if i not in feature_cols]\n",
    "temp_new = temp.drop(droplist,axis=1)\n",
    "\n",
    "\n",
    "for column in temp_new.columns:\n",
    "        temp_new[column] = ((temp_new[column] - temp_new[column].min()) / (temp_new[column].max() - temp_new[column].min()))\n",
    "\n",
    "temp_new.fillna(0.0, inplace = True)\n",
    "\n",
    "temp_new=temp_new.applymap(lambda x: x + np.random.rand()/20.0)\n",
    "\n",
    "new = pd.concat([temp_new, bug],ignore_index=False, sort=False, axis=1) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "pd.plotting.parallel_coordinates(\n",
    "        new, 'Bugs', color=('blue', 'red'), alpha=0.5, axvlines=False\n",
    "    )\n",
    "# ax.get_legend().remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitness\n",
    "avg = []\n",
    "for i in range(num_generations):\n",
    "    av = sum(fit[i])/len(fit[i])\n",
    "    avg.append(av)\n",
    "\n",
    "print(avg)\n",
    "itr = list(range(num_generations))\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "ax.plot(avg,itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitness\n",
    "avg = []\n",
    "for i in range(num_generations):\n",
    "    av = max(fit[i])\n",
    "    avg.append(av)\n",
    "\n",
    "print(avg)\n",
    "itr = list(range(num_generations))\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "ax.plot(avg,itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "\n",
    "# df = weighted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['contains_bug'] = df['contains_bug'].apply(lambda x : 1 if x != 0 else x)\n",
    "X = data.iloc[:, 0:97]\n",
    "y = data.iloc[:, -1]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1,stratify = y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10)\n",
    "cv_results = cross_val_score(forest,X,y,cv=10)\n",
    "forest.fit(X, y)\n",
    "\n",
    "prediction = cross_val_predict(forest,X,y,cv=10)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y, prediction)\n",
    "print('Accuracy: ' ,accuracy)\n",
    "\n",
    "print(confusion_matrix(y, prediction))\n",
    "precision = precision_score(y, prediction)\n",
    "print('Precision: %.3f' % precision)\n",
    "recall = recall_score(y, prediction, average='binary')\n",
    "print('Recall: %.3f' % recall)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "recall_val = []\n",
    "for i in range(num_generations):\n",
    "    print('Plot number: ', i)\n",
    "    new_weighted_data = data.iloc[:,0:97].multiply(weight_list[i], axis=1)\n",
    "    df2 = new_weighted_data\n",
    "    y = data.iloc[:,-1]\n",
    "    X = new_weighted_data\n",
    "\n",
    "    forest2 = RandomForestClassifier(n_estimators=10)\n",
    "    cv_results = cross_val_score(forest2,X,y,cv=10)\n",
    "    forest2.fit(X, y)\n",
    "\n",
    "    prediction = cross_val_predict(forest2,X,y,cv=10)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y, prediction)\n",
    "    print('Accuracy: ' ,accuracy)\n",
    "\n",
    "    print(confusion_matrix(y, prediction))\n",
    "    precision = precision_score(y, prediction)\n",
    "    print('Precision: %.3f' % precision)\n",
    "    recall = recall_score(y, prediction, average='binary')\n",
    "    print('Recall: %.3f' % recall)\n",
    "    recall_val.append(recall)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = list(range(num_generations))\n",
    "zip_iterator = zip(itr, recall_val)\n",
    "a_dictionary = dict(zip_iterator)\n",
    "# print(a_dictionary)\n",
    "res_dict = dict(sorted(a_dictionary.items(), key=lambda item: item[1], reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorted Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = DataFrame(weight_list)\n",
    "df_weights.to_csv (r'softset_neo4j_weights2.csv', index = False, header=True)\n",
    "w = df_weights.iloc[57, 0:97]\n",
    "w = list(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = data.iloc[:, 0:97]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "forest = KNeighborsClassifier()\n",
    "cv_results = cross_val_score(forest,X,y,cv=10)\n",
    "forest.fit(X, y)\n",
    "\n",
    "prediction = cross_val_predict(forest,X,y,cv=10)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y, prediction)\n",
    "print('Accuracy: ' ,accuracy)\n",
    "\n",
    "print(confusion_matrix(y, prediction))\n",
    "precision = precision_score(y, prediction)\n",
    "print('Precision: %.3f' % precision)\n",
    "recall = recall_score(y, prediction, average='binary')\n",
    "print('Recall: %.3f' % recall)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_val = []\n",
    "for i in range(num_generations):\n",
    "    print('Plot number: ', i)\n",
    "    new_weighted_data = data.iloc[:,0:97].multiply(weight_list[i], axis=1)\n",
    "    df2 = new_weighted_data\n",
    "    y = data.iloc[:,-1]\n",
    "    X = new_weighted_data\n",
    "\n",
    "    forest2 = KNeighborsClassifier()\n",
    "    cv_results = cross_val_score(forest2,X,y,cv=10)\n",
    "    forest2.fit(X, y)\n",
    "\n",
    "    prediction = cross_val_predict(forest2,X,y,cv=10)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y, prediction)\n",
    "    print('Accuracy: ' ,accuracy)\n",
    "\n",
    "    print(confusion_matrix(y, prediction))\n",
    "    precision = precision_score(y, prediction)\n",
    "    print('Precision: %.3f' % precision)\n",
    "    recall = recall_score(y, prediction, average='binary')\n",
    "    print('Recall: %.3f' % recall)\n",
    "    recall_val.append(recall)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = list(range(num_generations))\n",
    "zip_iterator = zip(itr, recall_val)\n",
    "a_dictionary = dict(zip_iterator)\n",
    "# print(a_dictionary)\n",
    "res_dict = dict(sorted(a_dictionary.items(), key=lambda item: item[1], reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorted Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = data.iloc[:, 0:97]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "forest = LogisticRegression(random_state=0)\n",
    "cv_results = cross_val_score(forest,X,y,cv=10)\n",
    "forest.fit(X, y)\n",
    "\n",
    "prediction = cross_val_predict(forest,X,y,cv=10)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y, prediction)\n",
    "print('Accuracy: ' ,accuracy)\n",
    "\n",
    "print(confusion_matrix(y, prediction))\n",
    "precision = precision_score(y, prediction)\n",
    "print('Precision: %.3f' % precision)\n",
    "recall = recall_score(y, prediction, average='binary')\n",
    "print('Recall: %.3f' % recall)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_val = []\n",
    "for i in range(num_generations):\n",
    "    print('Plot number: ', i)\n",
    "    new_weighted_data = data.iloc[:,0:97].multiply(weight_list[i], axis=1)\n",
    "    df2 = new_weighted_data\n",
    "    y = data.iloc[:,-1]\n",
    "    X = new_weighted_data\n",
    "\n",
    "    forest2 = LogisticRegression(random_state=0)\n",
    "    cv_results = cross_val_score(forest2,X,y,cv=10)\n",
    "    forest2.fit(X, y)\n",
    "\n",
    "    prediction = cross_val_predict(forest2,X,y,cv=10)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y, prediction)\n",
    "    print('Accuracy: ' ,accuracy)\n",
    "\n",
    "    print(confusion_matrix(y, prediction))\n",
    "    precision = precision_score(y, prediction)\n",
    "    print('Precision: %.3f' % precision)\n",
    "    recall = recall_score(y, prediction, average='binary')\n",
    "    print('Recall: %.3f' % recall)\n",
    "    recall_val.append(recall)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "itr = list(range(num_generations))\n",
    "zip_iterator = zip(itr, recall_val)\n",
    "a_dictionary = dict(zip_iterator)\n",
    "# print(a_dictionary)\n",
    "res_dict = dict(sorted(a_dictionary.items(), key=lambda item: item[1], reverse = True))\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = data.iloc[:, 0:97]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "forest = GaussianNB()\n",
    "cv_results = cross_val_score(forest,X,y,cv=10)\n",
    "forest.fit(X, y)\n",
    "\n",
    "prediction = cross_val_predict(forest,X,y,cv=10)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y, prediction)\n",
    "print('Accuracy: ' ,accuracy)\n",
    "\n",
    "print(confusion_matrix(y, prediction))\n",
    "precision = precision_score(y, prediction)\n",
    "print('Precision: %.3f' % precision)\n",
    "recall = recall_score(y, prediction, average='binary')\n",
    "print('Recall: %.3f' % recall)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_val = []\n",
    "for i in range(num_generations):\n",
    "    print('Plot number: ', i)\n",
    "    new_weighted_data = data.iloc[:,0:97].multiply(weight_list[i], axis=1)\n",
    "    df2 = new_weighted_data\n",
    "    y = data.iloc[:,-1]\n",
    "    X = new_weighted_data\n",
    "\n",
    "    forest2 = GaussianNB()\n",
    "    cv_results = cross_val_score(forest2,X,y,cv=10)\n",
    "    forest2.fit(X, y)\n",
    "\n",
    "    prediction = cross_val_predict(forest2,X,y,cv=10)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y, prediction)\n",
    "    print('Accuracy: ' ,accuracy)\n",
    "\n",
    "    print(confusion_matrix(y, prediction))\n",
    "    precision = precision_score(y, prediction)\n",
    "    print('Precision: %.3f' % precision)\n",
    "    recall = recall_score(y, prediction, average='binary')\n",
    "    print('Recall: %.3f' % recall)\n",
    "    recall_val.append(recall)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "itr = list(range(num_generations))\n",
    "zip_iterator = zip(itr, recall_val)\n",
    "a_dictionary = dict(zip_iterator)\n",
    "# print(a_dictionary)\n",
    "res_dict = dict(sorted(a_dictionary.items(), key=lambda item: item[1], reverse = True))\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weighted_data = data.iloc[:,0:97].multiply(w)\n",
    "weight_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= new_weighted_data\n",
    "y=data.iloc[:,-1]\n",
    "data2 = pd.concat([x, y], ignore_index=False, sort=False, axis=1)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=20.0)\n",
    "tsne_obj= tsne.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df1 = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                            'Y':tsne_obj[:,1],\n",
    "                           'bug':data.iloc[:,-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"X\", y=\"Y\",hue=\"bug\",data=tsne_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Neo4j Dataset\", fontsize=15)\n",
    "sns.scatterplot(x=\"X\", y=\"Y\",hue=\"bug\",data=tsne_df1,alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= data.iloc[:, 0:97]\n",
    "y=data.iloc[:,-1]\n",
    "data = pd.concat([x, y], ignore_index=False, sort=False, axis=1)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=10.0)\n",
    "tsne_obj= tsne.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df1 = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                            'Y':tsne_obj[:,1],\n",
    "                           'bug':data.iloc[:,-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Original Neo4j Dataset\", fontsize=15)\n",
    "sns.scatterplot(x=\"X\", y=\"Y\",hue=\"bug\",data=tsne_df1, alpha = 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
